"""
æ‰‹åŠ¿è¯†åˆ«å™¨ - ä½¿ç”¨ Google MediaPipe Gesture Recognizer Task
å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯†åˆ«å‡†ç¡®åº¦æ›´é«˜
"""

from enum import Enum, auto
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import os


class GestureType(Enum):
    """æ‰‹åŠ¿ç±»å‹æšä¸¾ - å¯¹åº” MediaPipe å®˜æ–¹æ‰‹åŠ¿"""
    NONE = auto()           # æ— è¯†åˆ«æ‰‹åŠ¿
    FIST = auto()           # Closed_Fist æ¡æ‹³ âœŠ
    OPEN_PALM = auto()      # Open_Palm å¼ å¼€æ‰‹æŒ ğŸ–ï¸
    POINTING_UP = auto()    # Pointing_Up å‘ä¸ŠæŒ‡ â˜ï¸
    VICTORY = auto()        # Victory åŒæŒ‡ âœŒï¸
    I_LOVE_YOU = auto()     # ILoveYou ä¸‰æŒ‡ ğŸ¤Ÿ
    THUMB_UP = auto()       # Thumb_Up å¤§æ‹‡æŒ‡ ğŸ‘
    THUMB_DOWN = auto()     # Thumb_Down å¤§æ‹‡æŒ‡å‘ä¸‹ ğŸ‘


# MediaPipe æ‰‹åŠ¿åç§°åˆ°æˆ‘ä»¬æšä¸¾çš„æ˜ å°„
GESTURE_MAP = {
    'Closed_Fist': GestureType.FIST,
    'Open_Palm': GestureType.OPEN_PALM,
    'Pointing_Up': GestureType.POINTING_UP,
    'Victory': GestureType.VICTORY,
    'ILoveYou': GestureType.I_LOVE_YOU,
    'Thumb_Up': GestureType.THUMB_UP,
    'Thumb_Down': GestureType.THUMB_DOWN,
}


class GestureRecognizer:
    """ä½¿ç”¨ MediaPipe Gesture Recognizer Task çš„æ‰‹åŠ¿è¯†åˆ«å™¨"""

    SMOOTHING_FRAMES = 3  # å¹³æ»‘çª—å£ï¼š3 å¸§ï¼ˆä» 4 é™åˆ° 3ï¼Œæ›´å¿«å“åº”ï¼‰

    def __init__(self, model_path='gesture_recognizer.task'):
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(
                f"Model not found: {model_path}\n"
                "Download: https://storage.googleapis.com/mediapipe-models/"
                "gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
            )

        base_options = python.BaseOptions(model_asset_path=model_path)
        options = vision.GestureRecognizerOptions(
            base_options=base_options,
            running_mode=vision.RunningMode.VIDEO,
            num_hands=1,
            min_hand_detection_confidence=0.5,  # ä» 0.6 â†’ 0.5ï¼Œæ›´å®¹æ˜“æ£€æµ‹åˆ°æ‰‹
            min_hand_presence_confidence=0.5,   # ä» 0.6 â†’ 0.5
            min_tracking_confidence=0.5,        # ä» 0.6 â†’ 0.5
        )
        self.recognizer = vision.GestureRecognizer.create_from_options(options)
        self.frame_count = 0
        # å¤šå¸§å¹³æ»‘
        from collections import deque
        self.gesture_history = deque(maxlen=self.SMOOTHING_FRAMES)
        self.confirmed_gesture = GestureType.NONE
        self.raw_gesture = GestureType.NONE
        self.raw_confidence = 0.0

    def recognize(self, frame, frame_width, frame_height):
        """è¯†åˆ«å½“å‰å¸§ä¸­çš„æ‰‹åŠ¿ï¼ˆå¸¦å¤šå¸§å¹³æ»‘ï¼‰"""
        import cv2

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)

        self.frame_count += 1
        timestamp_ms = int(self.frame_count * 33)

        result = self.recognizer.recognize_for_video(mp_image, timestamp_ms)

        # è§£æåŸå§‹æ‰‹åŠ¿
        raw_gesture = GestureType.NONE
        self.raw_confidence = 0.0
        points = {}

        if result.gestures and len(result.gestures) > 0:
            top_gesture = result.gestures[0][0]
            gesture_name = top_gesture.category_name
            confidence = top_gesture.score
            self.raw_confidence = confidence

            # ç½®ä¿¡åº¦é˜ˆå€¼ 0.5ï¼ˆä» 0.6 é™ä½ï¼Œæé«˜çµæ•åº¦ï¼‰
            if confidence > 0.5 and gesture_name in GESTURE_MAP:
                raw_gesture = GESTURE_MAP[gesture_name]

        self.raw_gesture = raw_gesture

        # å¤šå¸§å¹³æ»‘ - ä½¿ç”¨å¤šæ•°æŠ•ç¥¨ï¼ˆå¥½å“å‘³ï¼šä¸è¦æ±‚å…¨éƒ¨ç›¸åŒï¼‰
        self.gesture_history.append(raw_gesture)
        if len(self.gesture_history) >= self.SMOOTHING_FRAMES:
            # ç»Ÿè®¡æœ€å¸¸å‡ºç°çš„æ‰‹åŠ¿
            from collections import Counter
            gesture_counts = Counter(self.gesture_history)
            most_common_gesture, count = gesture_counts.most_common(1)[0]

            # å¤šæ•°æŠ•ç¥¨ï¼šè‡³å°‘ 2/3 å¸§ç›¸åŒï¼ˆ3 å¸§ä¸­è‡³å°‘ 2 å¸§ï¼‰
            if count >= (self.SMOOTHING_FRAMES * 2 // 3):
                self.confirmed_gesture = most_common_gesture

            # æ‰‹ç¦»å¼€æ—¶å¿«é€Ÿé‡ç½®ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
            if raw_gesture == GestureType.NONE:
                self.confirmed_gesture = GestureType.NONE

        # è·å–æ‰‹éƒ¨å…³é”®ç‚¹
        if result.hand_landmarks and len(result.hand_landmarks) > 0:
            landmarks = result.hand_landmarks[0]
            index_tip = landmarks[8]   # é£ŸæŒ‡å°–
            index_base = landmarks[5]  # é£ŸæŒ‡æ ¹éƒ¨
            wrist = landmarks[0]       # æ‰‹è…•

            # åˆ¤æ–­æ‰‹æŒ‡æ–¹å‘
            pointing_up = index_tip.y < index_base.y

            # æ£€æµ‹å•æŒ‡ä¼¸å‡ºï¼ˆç”¨äºå‘ä¸‹æŒ‡æ»šåŠ¨ï¼‰
            # é£ŸæŒ‡ä¼¸å‡ºï¼šæŒ‡å°–åˆ°æ‰‹è…•è·ç¦» > æŒ‡æ ¹åˆ°æ‰‹è…•è·ç¦»
            index_extended = self._distance(index_tip, wrist) > self._distance(index_base, wrist) * 1.1
            # å…¶ä»–æ‰‹æŒ‡æ”¶èµ·ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
            middle_tip = landmarks[12]
            middle_base = landmarks[9]
            middle_folded = self._distance(middle_tip, wrist) < self._distance(middle_base, wrist) * 1.5

            single_finger = index_extended and middle_folded

            points = {
                'index_x': int(index_tip.x * frame_width),
                'index_y': int(index_tip.y * frame_height),
                'pointing_up': pointing_up,
                'single_finger': single_finger,
            }

        return self.confirmed_gesture, points

    def _distance(self, p1, p2) -> float:
        """è®¡ç®—ä¸¤ç‚¹è·ç¦»"""
        return ((p1.x - p2.x)**2 + (p1.y - p2.y)**2) ** 0.5

    def get_debug_info(self):
        """è¿”å›è°ƒè¯•ä¿¡æ¯"""
        return f"Raw:{self.raw_gesture.name}({self.raw_confidence:.2f})"

    def close(self):
        """é‡Šæ”¾èµ„æº"""
        if hasattr(self, 'recognizer'):
            self.recognizer.close()

