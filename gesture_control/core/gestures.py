"""
æ‰‹åŠ¿è¯†åˆ«å™¨ - ä½¿ç”¨ Google MediaPipe Gesture Recognizer Task
å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯†åˆ«å‡†ç¡®åº¦æ›´é«˜
"""

from enum import Enum, auto
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import os


class GestureType(Enum):
    """æ‰‹åŠ¿ç±»å‹æšä¸¾ - å¯¹åº” MediaPipe å®˜æ–¹æ‰‹åŠ¿"""
    NONE = auto()           # æ— è¯†åˆ«æ‰‹åŠ¿
    FIST = auto()           # Closed_Fist æ¡æ‹³ âœŠ
    OPEN_PALM = auto()      # Open_Palm å¼ å¼€æ‰‹æŒ ğŸ–ï¸
    POINTING_UP = auto()    # Pointing_Up å‘ä¸ŠæŒ‡ â˜ï¸
    VICTORY = auto()        # Victory åŒæŒ‡ âœŒï¸
    I_LOVE_YOU = auto()     # ILoveYou ä¸‰æŒ‡ ğŸ¤Ÿ
    THUMB_UP = auto()       # Thumb_Up å¤§æ‹‡æŒ‡ ğŸ‘
    THUMB_DOWN = auto()     # Thumb_Down å¤§æ‹‡æŒ‡å‘ä¸‹ ğŸ‘


# MediaPipe æ‰‹åŠ¿åç§°åˆ°æˆ‘ä»¬æšä¸¾çš„æ˜ å°„
GESTURE_MAP = {
    'Closed_Fist': GestureType.FIST,
    'Open_Palm': GestureType.OPEN_PALM,
    'Pointing_Up': GestureType.POINTING_UP,
    'Victory': GestureType.VICTORY,
    'ILoveYou': GestureType.I_LOVE_YOU,
    'Thumb_Up': GestureType.THUMB_UP,
    'Thumb_Down': GestureType.THUMB_DOWN,
}


class GestureRecognizer:
    """ä½¿ç”¨ MediaPipe Gesture Recognizer Task çš„æ‰‹åŠ¿è¯†åˆ«å™¨"""

    SMOOTHING_FRAMES = 4  # è¿ç»­ N å¸§ç›¸åŒæ‰ç¡®è®¤

    def __init__(self, model_path='gesture_recognizer.task'):
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(
                f"Model not found: {model_path}\n"
                "Download: https://storage.googleapis.com/mediapipe-models/"
                "gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
            )

        base_options = python.BaseOptions(model_asset_path=model_path)
        options = vision.GestureRecognizerOptions(
            base_options=base_options,
            running_mode=vision.RunningMode.VIDEO,
            num_hands=1,
            min_hand_detection_confidence=0.6,
            min_hand_presence_confidence=0.6,
            min_tracking_confidence=0.6,
        )
        self.recognizer = vision.GestureRecognizer.create_from_options(options)
        self.frame_count = 0
        # å¤šå¸§å¹³æ»‘
        from collections import deque
        self.gesture_history = deque(maxlen=self.SMOOTHING_FRAMES)
        self.confirmed_gesture = GestureType.NONE
        self.raw_gesture = GestureType.NONE
        self.raw_confidence = 0.0

    def recognize(self, frame, frame_width, frame_height):
        """è¯†åˆ«å½“å‰å¸§ä¸­çš„æ‰‹åŠ¿ï¼ˆå¸¦å¤šå¸§å¹³æ»‘ï¼‰"""
        import cv2

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)

        self.frame_count += 1
        timestamp_ms = int(self.frame_count * 33)

        result = self.recognizer.recognize_for_video(mp_image, timestamp_ms)

        # è§£æåŸå§‹æ‰‹åŠ¿
        raw_gesture = GestureType.NONE
        self.raw_confidence = 0.0
        points = {}

        if result.gestures and len(result.gestures) > 0:
            top_gesture = result.gestures[0][0]
            gesture_name = top_gesture.category_name
            confidence = top_gesture.score
            self.raw_confidence = confidence

            # ç½®ä¿¡åº¦é˜ˆå€¼ 0.6
            if confidence > 0.6 and gesture_name in GESTURE_MAP:
                raw_gesture = GESTURE_MAP[gesture_name]

        self.raw_gesture = raw_gesture

        # å¤šå¸§å¹³æ»‘
        self.gesture_history.append(raw_gesture)
        if len(self.gesture_history) >= self.SMOOTHING_FRAMES:
            # æ£€æŸ¥æ˜¯å¦è¿ç»­ N å¸§ç›¸åŒ
            if all(g == raw_gesture for g in self.gesture_history):
                self.confirmed_gesture = raw_gesture
            elif raw_gesture == GestureType.NONE:
                # æ‰‹ç¦»å¼€æ—¶å¿«é€Ÿé‡ç½®
                self.confirmed_gesture = GestureType.NONE

        # è·å–æ‰‹éƒ¨å…³é”®ç‚¹
        if result.hand_landmarks and len(result.hand_landmarks) > 0:
            landmarks = result.hand_landmarks[0]
            index_tip = landmarks[8]
            palm = landmarks[9]
            points = {
                'index_x': int(index_tip.x * frame_width),
                'index_y': int(index_tip.y * frame_height),
                'palm_x': palm.x,
                'palm_y': palm.y,
            }

        return self.confirmed_gesture, points

    def get_debug_info(self):
        """è¿”å›è°ƒè¯•ä¿¡æ¯"""
        return f"Raw:{self.raw_gesture.name}({self.raw_confidence:.2f})"

    def close(self):
        """é‡Šæ”¾èµ„æº"""
        if hasattr(self, 'recognizer'):
            self.recognizer.close()

